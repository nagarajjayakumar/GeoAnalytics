{"paragraphs":[{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1503493756152_599844743","id":"20170823-130916_307361037","dateCreated":"2017-08-23T13:09:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2191","text":"%spark2\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.SQLContext\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.geotools.data.{DataStoreFinder, Query}\nimport org.geotools.factory.CommonFactoryFinder\nimport org.geotools.filter.text.ecql.ECQL\nimport org.locationtech.geomesa.hbase.data.HBaseDataStore\nimport org.locationtech.geomesa.spark.{GeoMesaSpark, GeoMesaSparkKryoRegistrator}\nimport org.opengis.feature.simple.SimpleFeature\n\n\nsc.version\n\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"testSpark\")\n      .config(\"spark.sql.crossJoin.enabled\", \"true\")\n      .config(\"zookeeper.znode.parent\", \"/hbase-unsecure\")\n      .config(\"spark.sql.autoBroadcastJoinThreshold\", 1024*1024*200)\n      .getOrCreate()\n\nval sc = sparkSession.sparkContext\nval sqlContext = new SQLContext(sc)\nsqlContext.udf.register(\"WIDTH_BUCKET_1\" , com.hortonworks.gc.udf.WidthBucket.widthBucket _)   \n\n\n// Site Exposure\nval featureTypeName = \"siteexposure_event\"\n\nval dataFrame = sparkSession.read\n      .format(\"geomesa\")\n      .options(Map(\"bigtable.table.name\" -> \"site_exposure_1M\"))\n      .option(\"geomesa.feature\", featureTypeName)\n      .load()\n     \ndataFrame.createOrReplaceTempView(featureTypeName)\n\n\nval hexzoomevent = \"hexzoomevent\"\n\n    val dataFrameHexZoom = sparkSession.read\n      .format(\"geomesa\")\n      .options(Map(\"bigtable.table.name\" -> \"hex_zoom\"))\n      .option(\"geomesa.feature\", hexzoomevent)\n      .load()\n\n    dataFrameHexZoom.createOrReplaceTempView(hexzoomevent)\n   \n\n //val sqlQuery = \"SELECT count(*)  FROM   siteexposure_event, hexzoomevent  WHERE  portfolio_id =51325 AND    st_isempty(geom) = false \"\n \n // Temporary View to create 20 ZOOM level between the site exposure and the Hex zoom \n val sqlQuery = \"SELECT portfolio_id,account_id,site_id,geom,st_x(geom) AS x,st_y(geom) AS y,zoom_id AS lod  FROM   siteexposure_event, hexzoomevent  WHERE  portfolio_id =51325 AND    st_isempty(geom) = false \"\n val resultDataFrame = sparkSession.sql(sqlQuery)\n resultDataFrame.createOrReplaceTempView(\"siteexp_51325_tmp\")\n\n// Temporary view for the   siteexp_51325_tmp_l1\n val sqlQuery_1 = \"SELECT se.portfolio_id,  se.account_id,  se.site_id,  se.lod,  se.geom,  se.x,   se.y,   aphex.zoom_id,  aphex.zoom_value     AS hex_radius,  20037507.0671618  AS origin_x,  19971868.8804086  AS origin_y   FROM  siteexp_51325_tmp AS se  JOIN hexzoomevent AS aphex ON se.lod=aphex.zoom_id limit 10 \"\n \n val resultDataFrame_1 = sparkSession.sql(sqlQuery_1)\n resultDataFrame_1.createOrReplaceTempView(\"siteexp_51325_tmp_l1\")\n \n// Temporary view for the   siteexp_51325_tmp_l2\n val sqlQuery_2 = \"SELECT l1.portfolio_id,   l1.account_id,   l1.site_id,   l1.lod,   l1.geom,   l1.x,   l1.y,   l1.zoom_id,   l1.hex_radius,   l1.origin_x,   l1.origin_y,   l1.x - l1.origin_x AS dx,  l1.y - l1.origin_y AS dy,  l1.hex_radius*cos(30.0*Pi()/180.0) AS h,  l1.hex_radius / 2.0 AS v from siteexp_51325_tmp_l1 as l1 \"\n \n val resultDataFrame_2 = sparkSession.sql(sqlQuery_2)\n resultDataFrame_2.createOrReplaceTempView(\"siteexp_51325_tmp_l2\")\n \n\n// Temporary view for the   siteexp_51325_tmp_l3\n val sqlQuery_3 = \"SELECT l2.portfolio_id,   l2.account_id,   l2.site_id,   l2.lod,   l2.geom,   l2.x,   l2.y,   l2.zoom_id,   l2.hex_radius,   l2.origin_x,   l2.origin_y,   l2.dx,   l2.dy,   l2.h,   l2.v,   floor(dy/(3.0*l2.v)) AS quad_row,  floor(dx/(2.0*l2.h)) AS quad_col,  floor(dy/(3.0*l2.v))%2 AS mod2_row from siteexp_51325_tmp_l2 as l2 \"\n \n val resultDataFrame_3 = sparkSession.sql(sqlQuery_3)\n resultDataFrame_3.createOrReplaceTempView(\"siteexp_51325_tmp_l3\")\n \n// Temporary view for the   siteexp_51325_tmp_l4\n val sqlQuery_4 = \"SELECT l3.portfolio_id,   l3.account_id,   l3.site_id,   l3.lod,   l3.geom,   l3.x,   l3.y,   l3.zoom_id,   l3.hex_radius,   l3.dx,   l3.dy,   l3.h,   l3.v,   l3.quad_row,   l3.quad_col,   l3.mod2_row,   CASE     WHEN l3.mod2_row <> 0  THEN l3.quad_col*(2.0 * l3.h) + l3.h + l3.origin_x ELSE l3.quad_col * (2.0 * l3.h) + 0.0 + l3.origin_x END AS center_x, l3.quad_row*(3.0*l3.v) +l3.origin_y AS center_y from siteexp_51325_tmp_l3 as l3 \"\n \n val resultDataFrame_4 = sparkSession.sql(sqlQuery_4)\n resultDataFrame_4.createOrReplaceTempView(\"siteexp_51325_tmp_l4\")\n \n// Temporary view for the   siteexp_51325_tmp_l5\n val sqlQuery_5 = \"SELECT l4.portfolio_id,   l4.account_id,   l4.site_id,   l4.lod,   l4.geom,   l4.x,   l4.y,   l4.zoom_id,   l4.hex_radius,   l4.dx,   l4.dy,   l4.h,   l4.v,   l4.quad_row,   l4.quad_col,   l4.mod2_row,   l4.center_x,   l4.center_y,   l4.x - l4.center_x AS q2x,   l4.y - l4.center_y AS q2y,   abs(l4.x - l4.center_x) AS abs_q2x,   abs(l4.y - l4.center_y) AS abs_q2y  from siteexp_51325_tmp_l4 as l4 \"\n \n val resultDataFrame_5 = sparkSession.sql(sqlQuery_5)\n resultDataFrame_5.createOrReplaceTempView(\"siteexp_51325_tmp_l5\")\n \n \n// Temporary view for the   siteexp_51325_tmp_l6\n val sqlQuery_6 = \"SELECT l5.portfolio_id,   l5.account_id,   l5.site_id,   l5.lod,   l5.geom,   l5.x,   l5.y,   l5.zoom_id,   l5.hex_radius,   l5.dx,   l5.dy,   l5.h,   l5.v,   l5.quad_row,   l5.quad_col,   l5.mod2_row,   l5.center_x,   l5.center_y,   l5.q2x,   l5.q2y,   l5.abs_q2x,   l5.abs_q2y,   atan2 (l5.q2x,l5.q2y) AS arctangent  from siteexp_51325_tmp_l5 as l5 \"\n \n val resultDataFrame_6 = sparkSession.sql(sqlQuery_6)\n resultDataFrame_6.createOrReplaceTempView(\"siteexp_51325_tmp_l6\")\n \n // Temporary view for the   siteexp_51325_tmp_l7\n val sqlQuery_7 = \"SELECT l6.portfolio_id,   l6.account_id,   l6.site_id,   l6.lod,   l6.quad_row,   l6.quad_col,   CAST(l6.mod2_row AS DOUBLE) AS mod2_row,   CASE     WHEN     l6.abs_q2x > l6.hex_radius OR l6.abs_q2y > l6.hex_radius     THEN false     ELSE ((l6.abs_q2x/l6.h)+(l6.abs_q2y/l6.v)) <= 2.0   END AS is_inside,   CASE    WHEN arctangent < 0    THEN (arctangent + (2.0 * Pi())) * 180.0 / Pi()    ELSE arctangent * 180.0 / Pi()   END   AS azimuth  from siteexp_51325_tmp_l6 as l6 \"\n \n val resultDataFrame_7 = sparkSession.sql(sqlQuery_7)\n resultDataFrame_7.createOrReplaceTempView(\"siteexp_51325_tmp_l7\")\n\n \n resultDataFrame_7.show\n\n      ","dateUpdated":"2017-08-23T14:08:01+0000","dateFinished":"2017-08-23T14:09:30+0000","dateStarted":"2017-08-23T14:08:01+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.SparkSession\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.hadoop.conf.Configuration\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql.{Row, SparkSession}\n\nimport org.geotools.data.{DataStoreFinder, Query}\n\nimport org.geotools.factory.CommonFactoryFinder\n\nimport org.geotools.filter.text.ecql.ECQL\n\nimport org.locationtech.geomesa.hbase.data.HBaseDataStore\n\nimport org.locationtech.geomesa.spark.{GeoMesaSpark, GeoMesaSparkKryoRegistrator}\n\nimport org.opengis.feature.simple.SimpleFeature\n\nres750: String = 2.1.1.2.6.1.0-129\n\nsparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4ff1009c\n\nsc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@9343f42\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1b53897e\n\nres753: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,LongType,Some(List(DoubleType, DoubleType, DoubleType, LongType)))\n\nfeatureTypeName: String = siteexposure_event\n\ndataFrame: org.apache.spark.sql.DataFrame = [__fid__: string, portfolio_id: bigint ... 107 more fields]\n\nhexzoomevent: String = hexzoomevent\n\ndataFrameHexZoom: org.apache.spark.sql.DataFrame = [__fid__: string, zoom_id: bigint ... 1 more field]\n\nsqlQuery: String = \"SELECT portfolio_id,account_id,site_id,geom,st_x(geom) AS x,st_y(geom) AS y,zoom_id AS lod  FROM   siteexposure_event, hexzoomevent  WHERE  portfolio_id =51325 AND    st_isempty(geom) = false \"\n\nresultDataFrame: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 5 more fields]\n\nsqlQuery_1: String = \"SELECT se.portfolio_id,  se.account_id,  se.site_id,  se.lod,  se.geom,  se.x,   se.y,   aphex.zoom_id,  aphex.zoom_value     AS hex_radius,  20037507.0671618  AS origin_x,  19971868.8804086  AS origin_y   FROM  siteexp_51325_tmp AS se  JOIN hexzoomevent AS aphex ON se.lod=aphex.zoom_id limit 10 \"\n\nresultDataFrame_1: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 9 more fields]\n\nsqlQuery_2: String = \"SELECT l1.portfolio_id,   l1.account_id,   l1.site_id,   l1.lod,   l1.geom,   l1.x,   l1.y,   l1.zoom_id,   l1.hex_radius,   l1.origin_x,   l1.origin_y,   l1.x - l1.origin_x AS dx,  l1.y - l1.origin_y AS dy,  l1.hex_radius*cos(30.0*Pi()/180.0) AS h,  l1.hex_radius / 2.0 AS v from siteexp_51325_tmp_l1 as l1 \"\n\nresultDataFrame_2: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 13 more fields]\n\nsqlQuery_3: String = \"SELECT l2.portfolio_id,   l2.account_id,   l2.site_id,   l2.lod,   l2.geom,   l2.x,   l2.y,   l2.zoom_id,   l2.hex_radius,   l2.origin_x,   l2.origin_y,   l2.dx,   l2.dy,   l2.h,   l2.v,   floor(dy/(3.0*l2.v)) AS quad_row,  floor(dx/(2.0*l2.h)) AS quad_col,  floor(dy/(3.0*l2.v))%2 AS mod2_row from siteexp_51325_tmp_l2 as l2 \"\n\nresultDataFrame_3: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 16 more fields]\n\nsqlQuery_4: String = \"SELECT l3.portfolio_id,   l3.account_id,   l3.site_id,   l3.lod,   l3.geom,   l3.x,   l3.y,   l3.zoom_id,   l3.hex_radius,   l3.dx,   l3.dy,   l3.h,   l3.v,   l3.quad_row,   l3.quad_col,   l3.mod2_row,   CASE     WHEN l3.mod2_row <> 0  THEN l3.quad_col*(2.0 * l3.h) + l3.h + l3.origin_x ELSE l3.quad_col * (2.0 * l3.h) + 0.0 + l3.origin_x END AS center_x, l3.quad_row*(3.0*l3.v) +l3.origin_y AS center_y from siteexp_51325_tmp_l3 as l3 \"\n\nresultDataFrame_4: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 16 more fields]\n\nsqlQuery_5: String = \"SELECT l4.portfolio_id,   l4.account_id,   l4.site_id,   l4.lod,   l4.geom,   l4.x,   l4.y,   l4.zoom_id,   l4.hex_radius,   l4.dx,   l4.dy,   l4.h,   l4.v,   l4.quad_row,   l4.quad_col,   l4.mod2_row,   l4.center_x,   l4.center_y,   l4.x - l4.center_x AS q2x,   l4.y - l4.center_y AS q2y,   abs(l4.x - l4.center_x) AS abs_q2x,   abs(l4.y - l4.center_y) AS abs_q2y  from siteexp_51325_tmp_l4 as l4 \"\n\nresultDataFrame_5: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 20 more fields]\n\nsqlQuery_6: String = \"SELECT l5.portfolio_id,   l5.account_id,   l5.site_id,   l5.lod,   l5.geom,   l5.x,   l5.y,   l5.zoom_id,   l5.hex_radius,   l5.dx,   l5.dy,   l5.h,   l5.v,   l5.quad_row,   l5.quad_col,   l5.mod2_row,   l5.center_x,   l5.center_y,   l5.q2x,   l5.q2y,   l5.abs_q2x,   l5.abs_q2y,   atan2 (l5.q2x,l5.q2y) AS arctangent  from siteexp_51325_tmp_l5 as l5 \"\n\nresultDataFrame_6: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 21 more fields]\n\nsqlQuery_7: String = \"SELECT l6.portfolio_id,   l6.account_id,   l6.site_id,   l6.lod,   l6.quad_row,   l6.quad_col,   CAST(l6.mod2_row AS DOUBLE) AS mod2_row,   CASE     WHEN     l6.abs_q2x > l6.hex_radius OR l6.abs_q2y > l6.hex_radius     THEN false     ELSE ((l6.abs_q2x/l6.h)+(l6.abs_q2y/l6.v)) <= 2.0   END AS is_inside,   CASE    WHEN arctangent < 0    THEN (arctangent + (2.0 * Pi())) * 180.0 / Pi()    ELSE arctangent * 180.0 / Pi()   END   AS azimuth  from siteexp_51325_tmp_l6 as l6 \"\n\nresultDataFrame_7: org.apache.spark.sql.DataFrame = [portfolio_id: bigint, account_id: string ... 7 more fields]\n+------------+----------+------------+---+--------+--------+--------+---------+------------------+\n|portfolio_id|account_id|     site_id|lod|quad_row|quad_col|mod2_row|is_inside|           azimuth|\n+------------+----------+------------+---+--------+--------+--------+---------+------------------+\n|       51325|     98916| 51000147290|  0|      -9|      -8|    -1.0|     true|14.384067379377923|\n|       51325|     98916|102000147290|  0|      -9|      -8|    -1.0|     true|14.384067413421807|\n|       51325|     98584| 51000146922|  0|      -9|      -8|    -1.0|     true|14.384069251654735|\n|       51325|     98584|102000146922|  0|      -9|      -8|    -1.0|     true|14.384069285675976|\n|       51325|     98813| 51000147177|  0|      -9|      -8|    -1.0|     true|14.384054015975721|\n|       51325|     98813|102000147177|  0|      -9|      -8|    -1.0|     true|14.384054050019627|\n|       51325|      1863| 51000003490|  0|      -9|      -8|    -1.0|     true|14.384052679317714|\n|       51325|      1863| 51000003506|  0|      -9|      -8|    -1.0|     true|14.384052470565406|\n|       51325|      1863|102000003490|  0|      -9|      -8|    -1.0|     true|14.384052714045252|\n|       51325|      1863|153000003490|  0|      -9|      -8|    -1.0|     true|14.384052748089152|\n+------------+----------+------------+---+--------+--------+--------+---------+------------------+\n\n"}]}},{"text":"%spark2\n","user":"admin","dateUpdated":"2017-08-23T13:14:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1503494078981_868290561","id":"20170823-131438_1898822307","dateCreated":"2017-08-23T13:14:38+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2261"}],"name":"UseCase_2","id":"2CQGAV654","angularObjects":{"2CQ2GZDDE:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CQBK1CKC:shared_process":[],"2CNW8AS1D:shared_process":[],"2CMY8N527:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}