{"paragraphs":[{"text":"%spark2\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.SQLContext\n\nsc.version\n\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"testSpark\")\n      .config(\"spark.sql.crossJoin.enabled\", \"true\")\n      .config(\"zookeeper.znode.parent\", \"/hbase-unsecure\")\n      .getOrCreate()\n\nval dataFrame = sparkSession.read\n      .format(\"geomesa\")\n      .options(Map(\"bigtable.table.name\" -> \"siteexposure_1M\"))\n      .option(\"geomesa.feature\", \"event\")\n      .load()\n      \nval sc = sparkSession.sparkContext\nval sqlContext = new SQLContext(sc)\n\n\nsqlContext.udf.register(\"WIDTH_BUCKET\", (expr: Double, minValue: Double, maxValue: Double, numBucket: Long) =>  {\n\n      if (numBucket <= 0) {\n        throw new RuntimeException(s\"The num of bucket must be greater than 0, but got ${numBucket}\")\n      }\n\n      val lower: Double = Math.min(minValue, maxValue)\n      val upper: Double = Math.max(minValue, maxValue)\n\n      val result: Long = if (expr < lower) {\n        0\n      } else if (expr >= upper) {\n        numBucket + 1L\n      } else {\n        (numBucket.toDouble * (expr - lower) / (upper - lower) + 1).toLong\n      }\n\n      if (minValue > maxValue) (numBucket - result) + 1 else result\n    }\n    )\n\nval siteLossAnalyzFeatureTypeName = \"sitelossanalyzevent\"\nval featureTypeName = \"event\"\nval geom = \"geom\"\n\ndataFrame.createOrReplaceTempView(featureTypeName)\n\n    val dataFrameSiteLossAnalyz = sparkSession.read\n      .format(\"geomesa\")\n      .options(Map(\"bigtable.table.name\" -> \"sitelossanalysis\"))\n      .option(\"geomesa.feature\", siteLossAnalyzFeatureTypeName)\n      .load()\n\n    dataFrameSiteLossAnalyz.createOrReplaceTempView(siteLossAnalyzFeatureTypeName)\n    dataFrameSiteLossAnalyz.printSchema\n    \n    //val sqlQuery =\n     // \"select count(*) from event \"\n\n\n    // Query against the \"event\" schema\n   val sqlQuery =\n      \"select event.site_id, sum(event.s_udf_met1), avg(event.s_udf_met1), sum (sitelossanalyzevent.gross_loss), WIDTH_BUCKET(200.00 ,150.000000, 3990682000.000000, 99) as bin from event  LEFT OUTER JOIN sitelossanalyzevent ON event.site_id = sitelossanalyzevent.site_id  where st_intersects(st_makeBBOX(-94.0, 31.0, -96.0, 29.0), geom) group by event.site_id\"\n\n   // val sqlQuery =\n     // \"select event.portfolio_id, sum(event.s_udf_met1), avg(event.s_udf_met1), sum (sitelossanalyzevent.gross_loss) from event  LEFT OUTER JOIN sitelossanalyzevent ON event.site_id = //sitelossanalyzevent.site_id    group by event.portfolio_id\"\n\n    val resultDataFrame = sparkSession.sql(sqlQuery)\n    \n    \n\n    resultDataFrame.show\n      ","user":"admin","dateUpdated":"2017-08-01T14:46:10+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":334,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.SparkSession\n\nimport org.apache.spark.sql.SQLContext\n\nres145: String = 2.1.1.2.6.1.0-129\n\nsparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7da68c45\n\ndataFrame: org.apache.spark.sql.DataFrame = [__fid__: string, portfolio_id: bigint ... 107 more fields]\n\nsc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@5346c079\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@291630e9\n\nres151: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,LongType,Some(List(DoubleType, DoubleType, DoubleType, LongType)))\n\nsiteLossAnalyzFeatureTypeName: String = sitelossanalyzevent\n\nfeatureTypeName: String = event\n\ngeom: String = geom\n\ndataFrameSiteLossAnalyz: org.apache.spark.sql.DataFrame = [__fid__: string, portfolio_id: bigint ... 13 more fields]\nroot\n |-- __fid__: string (nullable = false)\n |-- portfolio_id: long (nullable = true)\n |-- analysis_id: long (nullable = true)\n |-- site_id: string (nullable = true)\n |-- account_id: string (nullable = true)\n |-- vendor_id: long (nullable = true)\n |-- model_id: long (nullable = true)\n |-- peril_id: long (nullable = true)\n |-- gross_loss: double (nullable = true)\n |-- damage_loss: double (nullable = true)\n |-- net_loss: double (nullable = true)\n |-- occ_id: long (nullable = true)\n |-- occ_ind: string (nullable = true)\n |-- const_id: long (nullable = true)\n |-- const_ind: string (nullable = true)\n\n\nsqlQuery: String = select event.site_id, sum(event.s_udf_met1), avg(event.s_udf_met1), sum (sitelossanalyzevent.gross_loss), WIDTH_BUCKET(200.00 ,150.000000, 3990682000.000000, 99) as bin from event  LEFT OUTER JOIN sitelossanalyzevent ON event.site_id = sitelossanalyzevent.site_id  where st_intersects(st_makeBBOX(-94.0, 31.0, -96.0, 29.0), geom) group by event.site_id\n\nresultDataFrame: org.apache.spark.sql.DataFrame = [site_id: string, sum(s_udf_met1): double ... 3 more fields]\n+------------+---------------+---------------+---------------+---+\n|     site_id|sum(s_udf_met1)|avg(s_udf_met1)|sum(gross_loss)|bin|\n+------------+---------------+---------------+---------------+---+\n|102000001941|      4150500.0|      4150500.0|1241.5140513097|  1|\n|102000010719|      544444.44|      544444.44|255.77616049343|  1|\n|102000070644|       400000.0|       400000.0|214.08976491726|  1|\n|102000098389|        73000.0|        73000.0|28.008719763507|  1|\n|102000099614|        70000.0|        70000.0|38.799149676773|  1|\n|102000099775|       196000.0|       196000.0|201.96445286735|  1|\n|102000099861|        70000.0|        70000.0| 23.13330829667|  1|\n|102000099870|            0.0|            0.0|13.983345821187|  1|\n|102000100429|        41000.0|        41000.0|90.838329556166|  1|\n|102000109367|        83000.0|        83000.0|92.484519526286|  1|\n|102000109656|        86000.0|        86000.0|78.660956134003|  1|\n|102027581359|            0.0|            0.0|           null|  1|\n|102027602612|            0.0|            0.0|           null|  1|\n|102027620273|            0.0|            0.0|           null|  1|\n|102027632847|            0.0|            0.0|           null|  1|\n|102027633010|            0.0|            0.0|           null|  1|\n|102027634887|            0.0|            0.0|           null|  1|\n|102027642800|            0.0|            0.0|           null|  1|\n|102027645289|            0.0|            0.0|           null|  1|\n|102027645402|            0.0|            0.0|           null|  1|\n+------------+---------------+---------------+---------------+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1501508957114_1423672006","id":"20170731-134917_1249912461","dateCreated":"2017-07-31T13:49:17+0000","dateStarted":"2017-08-01T14:45:59+0000","dateFinished":"2017-08-01T14:46:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2063"},{"text":"%dep\nz.addRepo(\"Spark Packages Repo\").url(\"https://repo.locationtech.org/content/repositories/geomesa-releases/\")\nz.load(\"org.locationtech.geomesa:geomesa-hbase_2.11:1.3.2\")\n\n\n      \n","user":"admin","dateUpdated":"2017-07-31T13:51:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Must be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter"}]},"apps":[],"jobName":"paragraph_1501509035513_1356279652","id":"20170731-135035_925714213","dateCreated":"2017-07-31T13:50:35+0000","dateStarted":"2017-07-31T13:51:47+0000","dateFinished":"2017-07-31T13:51:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2064"},{"text":"%dep\n","user":"admin","dateUpdated":"2017-07-31T13:51:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501509107250_-1445569108","id":"20170731-135147_1185690260","dateCreated":"2017-07-31T13:51:47+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2065"}],"name":"Naga_Jay_GeoAnalytics","id":"2CNU25ZRV","angularObjects":{"2CQ2GZDDE:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CQBK1CKC:shared_process":[],"2CNW8AS1D:shared_process":[],"2CMY8N527:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}